<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Face Registration - Auto Capture</title>
    <style>
        /* (Keep existing styles, or refine as needed) */
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.6; background-color: #f8f9fa; padding: 20px; font-size: 17px; display: flex; flex-direction: column; align-items: center; min-height: 100vh; }
        .container { max-width: 700px; width: 100%; margin: 20px auto; padding: 25px; background: white; border-radius: 12px; box-shadow: 0 5px 15px rgba(0,0,0,0.1); }
        .header { text-align: center; margin-bottom: 25px; }
        .header h1 { color: #333; }
        .header p { color: #666; font-size: 0.95em; }

        .video-container { width: 100%; max-width: 560px; margin: 15px auto; position: relative; border-radius: 8px; overflow: hidden; box-shadow: 0 2px 8px rgba(0,0,0,0.1); background-color: #000;}
        #videoElement { width: 100%; height: auto; display: block; transform: scaleX(-1); /* Mirror display */}

        .instructions-container { text-align: center; margin: 20px 0; padding: 15px; background-color: #e9ecef; border-radius: 8px; min-height: 80px;}
        #instructionText { font-size: 1.2em; color: #2c3e50; font-weight: bold; min-height: 1.5em; }
        #countdownText { font-size: 1.1em; color: #27ae60; margin-top: 5px; font-weight: bold;}
        #errorMessage { font-size: 0.9em; color: #e74c3c; margin-top: 5px; min-height: 1.2em;}


        .progress-container { width: 100%; max-width: 560px; margin: 15px auto; text-align: center; }
        #overallProgressText { font-size: 0.9em; color: #555; margin-bottom: 5px; }
        .progress-bar { width: 100%; height: 22px; background-color: #e0e0e0; border-radius: 10px; overflow: hidden; }
        .progress { width: 0%; height: 100%; background-color: #3498db; transition: width 0.4s ease-in-out; display: flex; align-items: center; justify-content: center; color: white; font-size: 0.8em;}

        .status { text-align: center; margin: 15px 0; padding: 10px; border-radius: 5px; display: none; font-weight: 500; }
        .status.success { background-color: #d4edda; color: #155724; border: 1px solid #c3e6cb; display: block; }
        .status.error { background-color: #f8d7da; color: #721c24; border: 1px solid #f5c6cb; display: block; }
        .status.info { background-color: #cce5ff; color: #004085; border: 1px solid #b8daff; display: block; }

        .controls { display: flex; justify-content: center; gap: 15px; margin-top: 25px; flex-wrap: wrap; }
        .button { padding: 12px 25px; font-size: 1.05em; border: none; border-radius: 6px; cursor: pointer; transition: background-color 0.2s ease, transform 0.1s ease; font-weight: 500; }
        .button:disabled { background-color: #ccc !important; cursor: not-allowed; color: #666 !important;}
        .button:hover:not(:disabled) { transform: translateY(-1px); }
        #startButton { background-color: #3498db; color: white; }
        #startButton:hover:not(:disabled) { background-color: #2980b9; }
        /* Capture button might be removed or repurposed if auto-capture is primary */
        #captureButton { background-color: #e67e22; color: white; display: none; /* Hidden for auto-capture focus */}
        #submitButton { background-color: #5cb85c; color: white; }
        #submitButton:hover:not(:disabled) { background-color: #4cae4c; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Multi-Pose Face Registration</h1>
            <p>The system will attempt to automatically capture your face for each pose.</p>
        </div>

        <div class="video-container">
            <video id="videoElement" autoplay playsinline muted></video> <!-- Changed to <video> and muted -->
        </div>

        <div class="instructions-container">
            <div id="instructionText">Please start the camera.</div>
            <div id="countdownText"></div>
            <div id="errorMessage"></div>
        </div>

        <div class="progress-container">
            <div id="overallProgressText">Images Captured: 0 / {{ max_images }}</div>
            <div class="progress-bar">
                <div class="progress" id="captureProgress" style="width: 0%;">0%</div>
            </div>
        </div>

        <div id="statusMessage" class="status"></div> <!-- General status messages -->

        <div class="controls">
            <button id="startButton" class="button">Start Camera</button>
            <!-- Capture button can be a manual override or for testing -->
            <button id="captureButton" class="button" disabled>Manual Capture (Debug)</button>
            <button id="submitButton" class="button" disabled>Submit Registration</button>
        </div>
    </div>

    <script>
        const videoElement = document.getElementById('videoElement');
        const startButton = document.getElementById('startButton');
        const captureButton = document.getElementById('captureButton'); // Manual capture
        const submitButton = document.getElementById('submitButton');
        
        const instructionText = document.getElementById('instructionText');
        const countdownText = document.getElementById('countdownText');
        const errorMessageElement = document.getElementById('errorMessage');
        const overallProgressText = document.getElementById('overallProgressText');
        const captureProgressBar = document.getElementById('captureProgress');
        const statusMessageElement = document.getElementById('statusMessage'); // Renamed for clarity

        const POSES = JSON.parse('{{ poses_json | safe }}');
        const MAX_IMAGES_TO_CAPTURE = parseInt('{{ max_images }}');
        const CLIENT_CAPTURE_DELAY_SECONDS = parseInt('{{ capture_delay_ms }}') / 1000; // From Flask, in seconds

        let clientStream = null;
        let currentPoseIndex = 0; // Client's view of current pose
        let totalImagesCapturedByClient = 0; // Client's count, server is source of truth via SSE
        let isCameraActive = false;
        
        let faceDetectionIntervalId = null;
        let faceDetectedConsistentlyTimerId = null;
        let faceDetectionDebounce = null; // For face_detected POST
        const FACE_DETECTION_INTERVAL_MS = 500; // How often to check for face on client
        const FACE_POST_DEBOUNCE_MS = 700; // How often to POST face detection status


        // --- Client-Side Face Detection (Simplified, using dummy for now) ---
        // For actual client-side detection, you'd integrate a library like face-api.js
        // This is a placeholder to simulate client-side detection logic
        async function isClientFaceDetected() {
            // Placeholder: In a real app, use face-api.js or similar on the videoElement
            // For now, let's assume a face is detected if camera is on.
            // More advanced: check if videoElement has valid data / brightness etc.
            return videoElement.readyState >= videoElement.HAVE_CURRENT_DATA && videoElement.videoWidth > 0;
        }

        async function notifyServerFaceDetectionStatus(detected) {
            if (totalImagesCapturedByClient >= MAX_IMAGES_TO_CAPTURE || !isCameraActive) return;
            try {
                // console.log(`Notifying server: face detected = ${detected}, pose_index = ${currentPoseIndex}`);
                const response = await fetch("{{ url_for('face_detected_registration') }}", {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ detected: detected, pose_index: currentPoseIndex })
                });
                if (response.ok) {
                    const data = await response.json();
                    // console.log("Server response to face detection: ", data);
                    if (data.action === "trigger_capture") {
                        // Server says conditions met, client should now take picture
                        console.log("Server instructed client to trigger capture for pose:", data.pose_name);
                        await performCapture();
                    }
                    // Instruction updates will primarily come from SSE for smoother UI
                } else {
                    console.error("Error notifying server of face detection:", response.status);
                }
            } catch (error) {
                console.error("Failed to send face detection status:", error);
            }
        }


        async function performCapture(isManual = false) {
            if (!isCameraActive || totalImagesCapturedByClient >= MAX_IMAGES_TO_CAPTURE) return;

            // Disable manual capture button during any capture attempt
            captureButton.disabled = true;
            showStatus("Capturing...", "info", 2000);
            instructionText.textContent = `Capturing for ${POSES[currentPoseIndex % POSES.length].toUpperCase()}...`;

            const canvas = document.createElement('canvas');
            canvas.width = videoElement.videoWidth;
            canvas.height = videoElement.videoHeight;
            const ctx = canvas.getContext('2d');
            // Flip the image back to normal before sending (because display is mirrored)
            ctx.translate(canvas.width, 0);
            ctx.scale(-1, 1);
            ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
            ctx.setTransform(1, 0, 0, 1, 0, 0); // Reset transform


            try {
                const response = await fetch("{{ url_for('upload_face') }}", {
                    method: 'POST',
                    body: canvas.toDataURL('image/jpeg'),
                    headers: { 'X-Current-Pose-Name': POSES[currentPoseIndex % POSES.length] } // Inform server of current pose
                });
                const result = await response.json();

                if (response.ok && result.status === 'success') {
                    // totalImagesCapturedByClient will be updated by SSE from server's total_images_captured
                    // This prevents desync if a capture fails but client thought it succeeded.
                    // For now, let's optimistically update client, but rely on SSE.
                    // totalImagesCapturedByClient = result.images_captured;
                    // updateOverallProgress();
                    
                    // Client advances its local pose index for next instruction guidance
                    // Server state for current_pose_index in registration_process_state will be updated by
                    // the 'face_detected_registration' POST if client sends its current pose index
                    currentPoseIndex = (currentPoseIndex + 1) % POSES.length;

                    // Reset auto-capture related flags on client too
                    clearTimeout(faceDetectedConsistentlyTimerId);
                    faceDetectedConsistentlyTimerId = null;
                    countdownText.textContent = "";


                    showStatus(result.message || `Image capture attempt sent!`, 'success');
                    // Next instruction will come from SSE based on server's updated state.
                } else {
                    showStatus(result.message || 'Failed to process image on server.', 'error');
                }
            } catch (err) {
                console.error('Capture POST error:', err);
                showStatus('Error sending image: ' + err.message, 'error');
            } finally {
                 if (totalImagesCapturedByClient < MAX_IMAGES_TO_CAPTURE && isCameraActive) {
                    captureButton.disabled = false; // Re-enable manual capture
                }
            }
        }


        async function startClientCamera() {
            if (clientStream) { // Stop existing stream first
                clientStream.getTracks().forEach(track => track.stop());
            }
            try {
                clientStream = await navigator.mediaDevices.getUserMedia({
                    video: { width: { ideal: 640 }, height: { ideal: 480 }, facingMode: "user" }
                });
                videoElement.srcObject = clientStream;
                videoElement.onloadedmetadata = () => { // Ensure video is loaded before trying to play
                    videoElement.play().catch(e => console.error("Video play error:", e));
                };
                isCameraActive = true;
                startButton.textContent = 'Camera Active';
                startButton.disabled = true;
                captureButton.disabled = false; // Enable manual capture
                showStatus('Camera started. Follow instructions.', 'success');
                
                // Start client-side face detection polling loop
                if(faceDetectionIntervalId) clearInterval(faceDetectionIntervalId);
                faceDetectionIntervalId = setInterval(async () => {
                    if (!isCameraActive || totalImagesCapturedByClient >= MAX_IMAGES_TO_CAPTURE) {
                        if (faceDetectionIntervalId) clearInterval(faceDetectionIntervalId);
                        return;
                    }
                    const detected = await isClientFaceDetected(); // Your actual client-side detection
                    
                    // Debounce notifying server
                    clearTimeout(faceDetectionDebounce);
                    faceDetectionDebounce = setTimeout(() => {
                         notifyServerFaceDetectionStatus(detected);
                    }, FACE_POST_DEBOUNCE_MS);

                }, FACE_DETECTION_INTERVAL_MS);

            } catch (err) {
                console.error("getUserMedia error:", err);
                instructionText.textContent = "Could not access camera.";
                errorMessageElement.textContent = `Camera Error: ${err.name} - ${err.message}. Check permissions.`;
                showStatus(`Failed to start camera: ${err.message}`, 'error');
                isCameraActive = false;
            }
        }

        startButton.addEventListener('click', startClientCamera);
        captureButton.addEventListener('click', () => performCapture(true)); // Manual capture

        submitButton.addEventListener('click', async () => {
            submitButton.disabled = true;
            showStatus('Submitting registration...', 'info', 0);
            try {
                const response = await fetch("{{ url_for('complete_registration') }}", { method: 'POST' });
                const result = await response.json();
                if (response.ok && result.status === 'success') {
                    showStatus('Registration successful! Redirecting...', 'success');
                    if (faceDetectionIntervalId) clearInterval(faceDetectionIntervalId);
                    if (eventSource) eventSource.close();
                    setTimeout(() => { window.location.href = "{{ url_for('login') }}"; }, 2000);
                } else {
                    showStatus(result.message || 'Registration failed.', 'error');
                    submitButton.disabled = (totalImagesCapturedByClient < MAX_IMAGES_TO_CAPTURE);
                }
            } catch (err) {
                showStatus('Error submitting: ' + err.message, 'error');
                submitButton.disabled = (totalImagesCapturedByClient < MAX_IMAGES_TO_CAPTURE);
            }
        });

        function updateOverallUIProgress(serverTotalCaptured) {
            totalImagesCapturedByClient = serverTotalCaptured; // Update client's count from server
            const percentage = (totalImagesCapturedByClient / MAX_IMAGES_TO_CAPTURE) * 100;
            captureProgressBar.style.width = `${percentage}%`;
            captureProgressBar.textContent = `${Math.round(percentage)}%`;
            overallProgressText.textContent = `Images Captured: ${totalImagesCapturedByClient} / ${MAX_IMAGES_TO_CAPTURE}`;

            if (totalImagesCapturedByClient >= MAX_IMAGES_TO_CAPTURE) {
                instructionText.textContent = "All images captured! Ready to submit.";
                captureButton.disabled = true; // Disable manual capture
                submitButton.disabled = false;
                if (faceDetectionIntervalId) clearInterval(faceDetectionIntervalId); // Stop polling
                // SSE might be closed by server or client here.
            } else {
                submitButton.disabled = true;
                captureButton.disabled = !isCameraActive; // Re-enable manual if camera is on
            }
        }

        // SSE Handling for instructions and progress
        let eventSource = null;
        function setupSSE() {
            if (eventSource) { eventSource.close(); }
            eventSource = new EventSource("{{ url_for('registration_status_feed') }}");
            console.log("SSE for /registration_status_feed connecting...");

            eventSource.onopen = () => console.log("SSE connection opened for registration status.");
            
            eventSource.onmessage = function(event) {
                try {
                    const data = JSON.parse(event.data);
                    // console.log("Reg SSE Data:", data);

                    if(data.instruction) instructionText.textContent = data.instruction;
                    errorMessageElement.textContent = data.error_message || "";
                    
                    // Server's countdown for auto-capture (display only)
                    if(data.auto_capture_countdown !== undefined && data.auto_capture_countdown !== null) {
                        countdownText.textContent = `Auto-capturing in: ${data.auto_capture_countdown}s`;
                    } else if (data.face_detected_for_auto_capture) {
                        countdownText.textContent = "Face detected, hold still...";
                    } else {
                        countdownText.textContent = ""; // Clear if no countdown
                    }

                    if (data.total_images_captured !== undefined) {
                        updateOverallUIProgress(data.total_images_captured);
                    }
                     // Update client's current_pose_index if server changes it,
                     // though client primarily drives this for its own UI.
                    if (data.current_pose_index !== undefined && data.current_pose_index !== currentPoseIndex && totalImagesCapturedByClient < MAX_IMAGES_TO_CAPTURE) {
                        // currentPoseIndex = data.current_pose_index; // This could cause jumps if client is ahead.
                        // Better: Server instructions imply the pose. Client follows instructions.
                    }

                    if(data.status === 'error' && data.error_message) {
                        showStatus(data.error_message, 'error');
                    } else if (data.status === 'complete') {
                        showStatus('Registration process complete! Redirecting...', 'success');
                         if (faceDetectionIntervalId) clearInterval(faceDetectionIntervalId);
                         if (eventSource) eventSource.close();
                         setTimeout(() => { window.location.href = "{{ url_for('login') }}"; }, 2000);
                    } else if (data.status === 'pending_submit') {
                        submitButton.disabled = false;
                        captureButton.disabled = true;
                        if (faceDetectionIntervalId) clearInterval(faceDetectionIntervalId);
                    }

                } catch (e) { console.error("Error parsing reg SSE:", e, "Data:", event.data); }
            };
            eventSource.onerror = function(err) {
                console.error("Reg EventSource error:", err);
                errorMessageElement.textContent = "Status update connection lost.";
                if (eventSource) eventSource.close();
                // Optionally try to reconnect
                // setTimeout(setupSSE, 5000);
            };
        }
        
        // Initial UI setup
        updateOverallUIProgress(0); // Initialize progress display
        setupSSE();

        window.addEventListener('beforeunload', () => {
            if (clientStream) clientStream.getTracks().forEach(track => track.stop());
            if (faceDetectionIntervalId) clearInterval(faceDetectionIntervalId);
            clearTimeout(faceDetectionDebounce);
            clearTimeout(faceDetectedConsistentlyTimerId);
            if (eventSource && eventSource.readyState !== EventSource.CLOSED) eventSource.close();
        });
    </script>
</body>
</html>